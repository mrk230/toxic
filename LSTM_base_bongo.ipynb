{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "train = pd.read_csv('../../data/train.csv')\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variables are in the training set itself so we need to split them up, into X and Y sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test[\"comment_text\"]\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach that we are taking is to feed the comments into the LSTM as part of the neural network but we can't just feed the words as it is.\n",
    "\n",
    "So this is what we are going to do:\n",
    "\n",
    "    Tokenization - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n",
    "    \n",
    "    Indexing - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n",
    "    \n",
    "    Index Representation- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into our LSTM. For eg, [1,2,3,4,2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[412, 437, 73, 134, 14, 249, 2, 71, 314, 78, 50, 9, 13, 626, 8, 2284, 492, 502, 102, 4, 611, 2, 35, 325, 126, 363, 3, 29, 38, 27, 52, 208, 2, 434, 57, 36, 1, 2394, 93, 1, 737, 468]\n"
     ]
    }
   ],
   "source": [
    "#for occurence of words\n",
    "#tokenizer.word_counts\n",
    "#for index of words\n",
    "#print(tokenizer.word_index)\n",
    "print(list_tokenized_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's still 1 problem! What if some comments are terribly long, while some are just 1 word? Wouldn't our indexed-sentence look like this:\n",
    "\n",
    "Comment #1: [8,9,3,7,3,6,3,6,3,6,2,3,4,9]\n",
    "\n",
    "Comment #2: [1,2]\n",
    "\n",
    "And we have to feed a stream of data that has a consistent length(fixed number of features) isn't it?\n",
    "\n",
    "\n",
    "We could make the shorter sentences as long as the others by filling the shortfall by zeros.But on the other hand, we also have to trim the longer ones to the same length(maxlen) as the short ones. In this case, we have set the max length to be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  412  437   73  134   14  249    2   71  314   78\n",
      "   50    9   13  626    8 2284  492  502  102    4  611    2   35  325\n",
      "  126  363    3   29   38   27   52  208    2  434   57   36    1 2394\n",
      "   93    1  737  468]\n"
     ]
    }
   ],
   "source": [
    "# play around\n",
    "print(X_t[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See best maxlen to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f27d7a9d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\n",
    "\n",
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 200, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,608,716\n",
      "Trainable params: 2,608,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "\n",
    "# empty space tells keras to infer automatically\n",
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "file_path = \"model_weights.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the short line of code that defines the LSTM layer, it's easy to miss the required input dimensions. LSTM takes in a tensor of [Batch Size, Time Steps, Number of Inputs]. Batch size is the number of samples in a batch, time steps is the number of recursion it runs for each input, or it could be pictured as the number of \"A\"s in the above picture. Lastly, number of inputs is the number of variables(number of words in each sentence in our case) you pass into LSTM\n",
    "\n",
    "We can make use of the output from the previous embedding layer which outputs a 3-D tensor of (None, 200, 128) into the LSTM layer. What it does is going through the samples, recursively run the LSTM model for 200 times, passing in the coordinates of the words each time. And because we want the unrolled version, we will receive a Tensor shape of (None, 200, 60), where 60 is the output dimension we have defined.\n",
    "\n",
    "Before we could pass the output to a normal layer, we need to reshape the 3D tensor into a 2D one. We reshape carefully to avoid throwing away data that is important to us, and ideally we want the resulting data to be a good representative of the original data.\n",
    "\n",
    "Therefore, we use a Global Max Pooling layer which is traditionally used in CNN problems to reduce the dimensionality of image data. In simple terms, we go through each patch of data, and we take the maximum values of each patch. These collection of maximum values will be a new set of down-sized data we can use.\n",
    "\n",
    "Finally, we feed the output into a Sigmoid layer. The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1227s 9ms/step - loss: 0.0710 - acc: 0.9776 - val_loss: 0.0498 - val_acc: 0.9816\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1225s 9ms/step - loss: 0.0452 - acc: 0.9831 - val_loss: 0.0475 - val_acc: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27cc58b8d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1,\n",
    "          callbacks=callbacks, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_te)\n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../data/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "\n",
    "sample_submission.to_csv(\"baseline_bongo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
