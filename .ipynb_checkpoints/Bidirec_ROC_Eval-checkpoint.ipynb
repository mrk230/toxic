{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 250, 200)          4000000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 250, 60)           62640     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 4,066,236\n",
      "Trainable params: 66,116\n",
      "Non-trainable params: 4,000,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "# set path/seeds\n",
    "sys.path.append('../')\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# %matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# custom imports\n",
    "from utils.custom_keras import toggle_train, all_layers_train, ROC_Eval\n",
    "from utils.metrics import accuracy\n",
    "from utils.metrics import columnwise_auc\n",
    "\n",
    "# get data\n",
    "train = pd.read_csv('../../data/train.csv')\n",
    "\n",
    "# train cutdown (just for testing purposes, can cut down amount of data here)\n",
    "train = train.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# seperate classes\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "\n",
    "\n",
    "## CREATE MY SPLIT HERE\n",
    "X_train, X_test, y_train, y_test= train_test_split(train, y, test_size = 0.1, \n",
    "                                                random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1,\n",
    "                                                 random_state=42)\n",
    "\n",
    "list_sentences_train = X_train[\"comment_text\"]\n",
    "list_sentences_val = X_val[\"comment_text\"]\n",
    "list_sentences_test = X_test[\"comment_text\"]\n",
    "\n",
    "# tokenize/index\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_val = tokenizer.texts_to_sequences(list_sentences_val)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "# get single sequence length\n",
    "maxlen = 250\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_val = pad_sequences(list_tokenized_val, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# get gembeddings and read into a dict (word: vector)\n",
    "# then use the embeddings to make embedding matrix, random init for ones not in vocab\n",
    "embedding_file = \"../glove.6B.200d.txt\"\n",
    "embed_size = 200\n",
    "\n",
    "embeddings_index = pd.read_table(embedding_file, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values)\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(len(word_index), max_features)\n",
    "\n",
    "# init with random ones for words not seen\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "\n",
    "\n",
    "# model building\n",
    "\n",
    "# first iteration, embeddings train = False, train for 2 epochs\n",
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "              name = 'embedding', trainable=False)(inp)\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer',\n",
    "         dropout=0.1, recurrent_dropout=0.1,\n",
    "          kernel_regularizer=regularizers.l2(0.001),\n",
    "          bias_regularizer=regularizers.l2(0.001))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(50, activation=\"relu\",\n",
    "          kernel_regularizer=regularizers.l2(0.001),\n",
    "          bias_regularizer=regularizers.l2(0.001))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12924 samples, validate on 1437 samples\n",
      "Epoch 1/2\n",
      "12864/12924 [============================>.] - ETA: 0s - loss: 0.4454 - acc: 0.9274\n",
      " ROC-AUC - score: 0.807847\n",
      "12924/12924 [==============================] - 85s 7ms/step - loss: 0.4444 - acc: 0.9277 - val_loss: 0.2678 - val_acc: 0.9632\n",
      "Epoch 2/2\n",
      "12864/12924 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9705\n",
      " ROC-AUC - score: 0.894225\n",
      "12924/12924 [==============================] - 85s 7ms/step - loss: 0.2203 - acc: 0.9705 - val_loss: 0.1843 - val_acc: 0.9696\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "batch_size = 64\n",
    "\n",
    "# call backs\n",
    "file_path = \"bidir_glov_reg001_embed6B200d_3e_multiTRAIN.hdf5\"\n",
    "es = EarlyStopping('val_loss', patience=5, mode=\"min\")\n",
    "msave = ModelCheckpoint(file_path, save_best_only=True)\n",
    "roc = ROC_Eval(X_val, y_val)\n",
    "\n",
    "# train everything but embedding\n",
    "hist = model.fit(X_t,y_train, batch_size=batch_size, epochs=2,\n",
    "          shuffle=True, callbacks=[es, msave, roc] , validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "# CAN LOWER THE LEARNING RATE MANUALLY ON THESE NEXT EPOCHS! ITS NOT LEARNING DICK I DONT THINK\n",
    "# EVEN PARTWAY THROUGH THE SECOND?\n",
    "\n",
    "# ALSO, COULD ALSO COMBINE THE LAST TWO? I.E. NOT HAVE A SEPERATE ONE WHERE WE JUST TRAIN\n",
    "# THE EMBEDDING LAYER, BUT JUST HAVE ONE WHERE WE TRAIN EVERYTHING?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # train only the embedding\n",
    "# model = toggle_train(model)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# model.fit(X_t,y_train, batch_size=batch_size, epochs=1,\n",
    "#           shuffle=True) # callbacks=callbacks , validation_data=(X_val, y_val)\n",
    "\n",
    "\n",
    "# # train everything\n",
    "# model = all_layers_train(model)\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# model.fit(X_t,y_train, batch_size=batch_size, epochs=1,\n",
    "#           shuffle=True) # callbacks=callbacks , validation_data=(X_val, y_val)\n",
    "\n",
    "\n",
    "# model.save(\"bidir_glov_reg001_embed6B200d_3e_multiTRAIN.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.26782124882036396, 0.18430817811623496], 'val_acc': [0.9632335930710131, 0.969612609452478], 'loss': [0.4443782192935371, 0.22026551213474033], 'acc': [0.927692660954079, 0.9704554782899605]}\n",
      "[0.8078472321660612, 0.8942250338662338]\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)\n",
    "print(roc.aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test data here X_te, y_test   Need to do the columwise AUC here and acc\n",
    "\n",
    "\n",
    "probs = model.predict(X_te)\n",
    "\n",
    "\n",
    "acc = accuracy(y_test, probs)\n",
    "print(\"acc\", acc)\n",
    "mean_col_auc = columnwise_auc(y_test, probs)\n",
    "print(\"mean col\", mean_col_auc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission prediction\n",
    "model.load_weights(\"bidir_glov_reg001_embed6B200d_3e_multiTRAIN.hdf5\")\n",
    "\n",
    "sub = pd.read_csv('../../data/test.csv')\n",
    "list_tokenized_sub = tokenizer.texts_to_sequences(sub[\"comment_text\"])\n",
    "X_sub = pad_sequences(list_tokenized_sub, maxlen=maxlen)\n",
    "\n",
    "y_sub = model.predict(X_sub)\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../data/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_sub\n",
    "\n",
    "sample_submission.to_csv(\"bidir_glov_reg001_embed6B200d_3e_multiTRAIN.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
