{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 200, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,608,956\n",
      "Trainable params: 2,608,836\n",
      "Non-trainable params: 120\n",
      "_________________________________________________________________\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "# set path/seeds\n",
    "sys.path.append('../')\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# %matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# get data\n",
    "train = pd.read_csv('../../data/train.csv')\n",
    "\n",
    "# train cutdown (just for testing purposes, can cut down amount of data here)\n",
    "train = train.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# seperate classes\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "\n",
    "## CREATE MY SPLIT HERE\n",
    "X_train, X_test, y_train, y_test= train_test_split(train, y, test_size = 0.1, \n",
    "                                                random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1,\n",
    "                                                 random_state=42)\n",
    "\n",
    "list_sentences_train = X_train[\"comment_text\"]\n",
    "list_sentences_val = X_val[\"comment_text\"]\n",
    "list_sentences_test = X_test[\"comment_text\"]\n",
    "\n",
    "# tokenize/index\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_val = tokenizer.texts_to_sequences(list_sentences_val)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "# get single sequence length\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_val = pad_sequences(list_tokenized_val, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "# model building\n",
    "\n",
    "embed_size = 128\n",
    "# empty space tells keras to infer automatically\n",
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# graveyard layers\n",
    "# x = Bidirectional(LSTM(60, return_sequences=True, name='lstm_layer'))(x)\n",
    "# Adam(lr=0.0001)\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "file_path = \"model_weights.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=50)\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25849 samples, validate on 2873 samples\n",
      "Epoch 1/2\n",
      "25849/25849 [==============================] - 232s 9ms/step - loss: 0.1226 - acc: 0.9631 - val_loss: 0.1024 - val_acc: 0.9723\n",
      "Epoch 2/2\n",
      "25849/25849 [==============================] - 227s 9ms/step - loss: 0.0560 - acc: 0.9803 - val_loss: 0.0697 - val_acc: 0.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdcbc346898>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "          shuffle=True) # callbacks=callbacks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test data here X_te, y_test   Need to do the columwise AUC here and acc\n",
    "from utils.metrics import accuracy\n",
    "from utils.metrics import columnwise_auc\n",
    "\n",
    "probs = model.predict(X_te)\n",
    "\n",
    "\n",
    "acc = accuracy(y_test, probs)\n",
    "print(\"acc\", acc)\n",
    "mean_col_auc = columnwise_auc(y_test, probs)\n",
    "print(\"mean col\", mean_col_auc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission prediction\n",
    "model.load_weights(file_path)\n",
    "\n",
    "sub = pd.read_csv('../../data/test.csv')\n",
    "y_sub = model.predict(sub)\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../data/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_sub\n",
    "\n",
    "sample_submission.to_csv(\"baseline_bongo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
